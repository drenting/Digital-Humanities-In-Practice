{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob\n",
    "import sys\n",
    "import os.path\n",
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataformat\n",
    "\n",
    "def data_format(big_file, name_file):\n",
    "    \"\"\"Make smaller txt file for each row in bigger txt file.\n",
    "    input: txt file, each row is a message\n",
    "    output: multiple txt files, each with one message\"\"\"\n",
    "    sorting = True\n",
    "    hold_lines = []\n",
    "    with open(big_file,'r', encoding='utf-8') as text_file:\n",
    "        for row in text_file:\n",
    "            hold_lines.append(row)\n",
    "    outer_count = 1\n",
    "    line_count = 0\n",
    "    while sorting:\n",
    "        count = 0\n",
    "        increment = (outer_count-1) * 1\n",
    "        left = len(hold_lines) - increment\n",
    "        small_file = name_file + str(outer_count * 1) + \".txt\"\n",
    "        hold_new_lines = []\n",
    "        if left < 1:\n",
    "            while count < left:\n",
    "                hold_new_lines.append(hold_lines[line_count])\n",
    "                count += 1\n",
    "                line_count += 1\n",
    "            sorting = False\n",
    "        else:\n",
    "            while count < 1:\n",
    "                hold_new_lines.append(hold_lines[line_count])\n",
    "                count += 1\n",
    "                line_count += 1\n",
    "        outer_count += 1\n",
    "        with open(small_file,'w', encoding='utf-8') as next_file:\n",
    "            for row in hold_new_lines:\n",
    "                next_file.write(row)\n",
    "                \n",
    "                \n",
    "\n",
    "#calling function for both categories\n",
    "data_format(big_file='relation.txt', name_file='pos_sw_training/relation/relation')\n",
    "data_format(big_file='no_relation.txt', name_file='pos_sw_training/no_relation/no_relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess \n",
    "\n",
    "def pre_process(filepath, new_filepath):\n",
    "    \"\"\"normalize, tokenize, stopword removal\"\"\"\n",
    "    for filename in glob.glob(filepath):\n",
    "        with open(filename, 'r', encoding='utf-8') as txtfile:\n",
    "            content = txtfile.read()\n",
    "            content = content.lower() #normalize\n",
    "            tokens = word_tokenize(content) #make into list of tokens\n",
    "            #tokens_without_sw = [word for word in tokens if not word in stopwords.words()] #remove stopwords\n",
    "        with open(os.path.join(new_filepath, os.path.basename(filename)), 'w', encoding='utf-8') as outfile:\n",
    "            for token in tokens: #for token in tokens_without_sw\n",
    "                outfile.write('%s ' % token)\n",
    "                \n",
    "\n",
    "#pre_process(filepath=\"training/relation/*.txt\", new_filepath='training/relation')\n",
    "#pre_process(filepath=\"training/no_relation/*.txt\", new_filepath='training/no_relation')\n",
    "pre_process(filepath=\"pos_sw_training/relation/*.txt\", new_filepath='pos_sw_training/relation')\n",
    "pre_process(filepath=\"pos_sw_training/no_relation/*.txt\", new_filepath='pos_sw_training/no_relation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS tagging\n",
    "\n",
    "## skip this step for BOW\n",
    "\n",
    "def pos_tagger(filepath, new_filepath):\n",
    "    \"\"\"replace tokens with pos tags and save to new file\"\"\"\n",
    "    for filename in glob.glob(filepath):\n",
    "        with open(filename, 'r', encoding='utf-8') as txtfile:\n",
    "            content = txtfile.read()\n",
    "            tokens = content.split()\n",
    "            tags = []\n",
    "            tagged = nltk.pos_tag(tokens) #pos tagging, tuples of token and tag\n",
    "            for tag in tagged: #tag is tuple of token and tag\n",
    "                if tag[0] == 'entity_1' or tag[0] == 'entity_2':\n",
    "                    tags.append(tag[0]) #leave entity_1 and entity_2\n",
    "                else:\n",
    "                    tags.append(tag[1]) #for all other words, use pos tag\n",
    "        with open(os.path.join(new_filepath, os.path.basename(filename)), 'w', encoding='utf-8') as outfile:\n",
    "            for tag in tags:\n",
    "                outfile.write('%s ' % tag)\n",
    "\n",
    "#pos_tagger(filepath=\"training/relation/*.txt\", new_filepath=\"pos_training/relation\")\n",
    "#pos_tagger(filepath=\"training/no_relation/*.txt\", new_filepath=\"pos_training/no_relation\")\n",
    "pos_tagger(filepath=\"pos_sw_training/relation/*.txt\", new_filepath=\"pos_sw_training/relation\")\n",
    "pos_tagger(filepath=\"pos_sw_training/no_relation/*.txt\", new_filepath=\"pos_sw_training/no_relation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading files\n",
    "\n",
    "#training_filepath = 'training' #for BOW\n",
    "#training_filepath = 'pos_training' #for pos\n",
    "training_filepath = 'pos_sw_training' #pos with stopwords\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "training_folder = cwd.joinpath(training_filepath)\n",
    "print('path:', training_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      training_folder.exists())\n",
    "\n",
    "# loading all files as training data.\n",
    "training_train = load_files(str(training_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspecting \n",
    "\n",
    "print(len(training_train.data))\n",
    "\n",
    "target_names = training_train.target_names # generated from subfolder names\n",
    "\n",
    "freqs = Counter(training_train.target)\n",
    "for category, frequency in freqs.items():\n",
    "    print(training_train.target_names[category], frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature representation with CountVectorizer\n",
    "\n",
    "ngram_range = (1,1) \n",
    "#ngram_range = (2,2)\n",
    "#ngram_range = (3,3)\n",
    "\n",
    "training_vec = CountVectorizer(ngram_range=ngram_range, encoding='utf-8')\n",
    "training_counts = training_vec.fit_transform(training_train.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training ans testing\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in skf.split(training_counts, training_train.target):\n",
    "    x_train, x_test = training_counts[train_index], training_counts[test_index]\n",
    "    y_train, y_test = training_train.target[train_index], training_train.target[test_index]\n",
    "    \n",
    "    #clf = MultinomialNB().fit(x_train, y_train)\n",
    "    clf = sklearn.linear_model.LogisticRegression(C=1e5)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred, target_names= ['no relation', 'relation']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
